{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00e8479",
   "metadata": {},
   "source": [
    "# Spark Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6fe6f",
   "metadata": {},
   "source": [
    "Working with the **vaccination_tweets.csv** file. This is also saved in the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d293f",
   "metadata": {},
   "source": [
    "Importing all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e056944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from s3fs.core import S3FileSystem\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a53724",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69fe0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local','TweetCleaningApp')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9e49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bucket = '686springbucket-emmanuel'\n",
    "my_key = 'Assignment6/vaccination_tweets.csv'\n",
    "\n",
    "s3 = S3FileSystem(anon=False)\n",
    "data = pd.read_csv(s3.open('{}/{}'.format(my_bucket, my_key), mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c858c",
   "metadata": {},
   "source": [
    "I then created a schema with the various data types for the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130b9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType([StructField(\"id\", StringType(),False),\\\n",
    "                       StructField(\"user_name\", StringType(),False),\\\n",
    "                       StructField(\"user_location\", StringType(),False),\\\n",
    "                       StructField(\"user_description\", StringType(),False),\\\n",
    "                       StructField(\"user_created\", StringType(),False),\\\n",
    "                       StructField(\"user_followers\", IntegerType(),False),\\\n",
    "                       StructField(\"user_friends\", IntegerType(),False),\\\n",
    "                       StructField(\"user_favourites\", IntegerType(),False),\\\n",
    "                       StructField(\"user_verified\", BooleanType(),False),\\\n",
    "                       StructField(\"date\", StringType(),False),\\\n",
    "                       StructField(\"text\", StringType(),False),\\\n",
    "                       StructField(\"hashtags\", StringType(),False),\\\n",
    "                       StructField(\"source\", StringType(),False),\\\n",
    "                       StructField(\"retweets\", IntegerType(),False),\\\n",
    "                       StructField(\"favorites\", IntegerType(),False),\\\n",
    "                       StructField(\"is_retweet\", BooleanType(),False)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21464d57",
   "metadata": {},
   "source": [
    "The spark data frame was created and the first 5 observations was printed to confirm everything works well so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4947fcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+-------------+--------------+------------+---------------+-------------+--------------+--------------------+--------------------+-------------------+--------+---------+----------+\n",
      "|         id|           user_name|       user_location|    user_description| user_created|user_followers|user_friends|user_favourites|user_verified|          date|                text|            hashtags|             source|retweets|favorites|is_retweet|\n",
      "+-----------+--------------------+--------------------+--------------------+-------------+--------------+------------+---------------+-------------+--------------+--------------------+--------------------+-------------------+--------+---------+----------+\n",
      "|1.34054E+18|          Rachel Roh|La Crescenta-Mont...|Aggregator of Asi...| 4/8/09 17:52|           405|        1692|           3247|        false| 12/20/20 6:06|Same folks said d...|  ['PfizerBioNTech']|Twitter for Android|       0|        0|     false|\n",
      "|1.33816E+18|         Albert Fong|   San Francisco, CA|Marketing dude, t...|9/21/09 15:27|           834|         666|            178|        false|12/13/20 16:27|While the world h...|                 NaN|    Twitter Web App|       1|        1|     false|\n",
      "|1.33786E+18|       eliðŸ‡±ðŸ‡¹ðŸ‡ªðŸ‡ºðŸ‘Œ|            Your Bed|     heil, hydra ðŸ–â˜º|6/25/20 23:30|            10|          88|            155|        false|12/12/20 20:33|#coronavirus #Spu...|['coronavirus', '...|Twitter for Android|       0|        0|     false|\n",
      "|1.33786E+18|       Charles Adler|Vancouver, BC - C...|Hosting \"CharlesA...|9/10/08 11:28|         49165|        3933|          21853|         true|12/12/20 20:23|Facts are immutab...|                 NaN|    Twitter Web App|     446|     2129|     false|\n",
      "|1.33785E+18|Citizen News Channel|                 NaN|Citizen News Chan...|4/23/20 17:58|           152|         580|           1473|        false|12/12/20 20:17|Explain to me aga...|['whereareallthes...| Twitter for iPhone|       0|        0|     false|\n",
      "+-----------+--------------------+--------------------+--------------------+-------------+--------------+------------+---------------+-------------+--------------+--------------------+--------------------+-------------------+--------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(data,schema=myschema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b6b583",
   "metadata": {},
   "source": [
    "Now we convert columns **date** to datetype because, the schema can not force a string to date, a new column called **N_date** was created which would be used for subsequent dataframe filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2d9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn(\"N_date\",to_date(col(\"date\"), \"MM/dd/yy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02dbf68",
   "metadata": {},
   "source": [
    "### Questions 1 - 3\n",
    "\n",
    "#### 1. The researchers only wants data between 2018-2020\n",
    "#### 2. The researcher only wants data where the tweeter has more than 0 friends\n",
    "#### 3. The researcher only wants tweets whose tweet text arenâ€™t marked as None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9664b",
   "metadata": {},
   "source": [
    "**NOTE**: The date filter was done based on the new date column **N_date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b946fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrd_data = df.filter(df['N_date']>='2018-01-01') and df.filter(df['N_date']<='2020-12-31')\n",
    "ftrd_data = ftrd_data.filter(ftrd_data['user_friends']>0)\n",
    "ftrd_data = ftrd_data.filter(ftrd_data['text']!='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe281e2d",
   "metadata": {},
   "source": [
    "#### 4. The researcher wants you to use regular expressions to remove all punctuation in the tweet text except exclamation marks and also wants you to remove all numbers from the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d65a28",
   "metadata": {},
   "source": [
    "The regex expression was applied to the **text** column based on the instruction in the question and a new column was created named **clnd_text**, the **text** column was also dropped and the first five rows of the **clnd_text** was printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ffcc52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           clnd_text|\n",
      "+--------------------+\n",
      "|Same folks said d...|\n",
      "|While the world h...|\n",
      "|coronavirus Sputn...|\n",
      "|Facts are immutab...|\n",
      "|Explain to me aga...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ftrd_data = ftrd_data.withColumn(\"clnd_text\",regexp_replace(ftrd_data['text'], r'[^!a-zA-Z\\s]','')).drop(ftrd_data[\"text\"])\n",
    "ftrd_data.select(ftrd_data[\"clnd_text\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec356a5",
   "metadata": {},
   "source": [
    "#### 5. Whatever is left after you have applied the above filters, the researcher wants you figure out the average number of retweets (round up) then tweets whose number of retweets are at the average mark â€œAVERAGEâ€ everything below the average mark â€œBELOW AVERAGEâ€ and everything above the average mark â€œABOVE AVERAGEâ€.  They want this data in another column marked â€œaverage_ratingâ€ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee4237",
   "metadata": {},
   "source": [
    "The average retweet was calculated and rounded up as instructed which was then assigned to the number **n** and then compared with the \"retweets\" column shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ffa235e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=ftrd_data.agg(ceil(mean(\"retweets\"))).collect()[0][0]\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b5418",
   "metadata": {},
   "source": [
    "The new colum **average_rating** was created as directed from question 5, I also printed the first 10 records, to confirm if the correct rating was given based on the **retweets** column showing the number of retweets, code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61063d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|average_rating|retweets|\n",
      "+--------------+--------+\n",
      "| BELOW AVERAGE|       0|\n",
      "| BELOW AVERAGE|       1|\n",
      "| BELOW AVERAGE|       0|\n",
      "| ABOVE AVERAGE|     446|\n",
      "| BELOW AVERAGE|       0|\n",
      "| BELOW AVERAGE|       0|\n",
      "| BELOW AVERAGE|       0|\n",
      "|       AVERAGE|       2|\n",
      "|       AVERAGE|       2|\n",
      "| BELOW AVERAGE|       0|\n",
      "+--------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ftrd_data = ftrd_data.withColumn('average_rating', when(col(\"retweets\")>n,\"ABOVE AVERAGE\")\\\n",
    "                                 .when(col(\"retweets\")==n,\"AVERAGE\")\\\n",
    "                                .otherwise(\"BELOW AVERAGE\"))\n",
    "ftrd_data.select(ftrd_data[\"average_rating\"], ftrd_data[\"retweets\"]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b2bb5",
   "metadata": {},
   "source": [
    "#### 6. Using the hashtags column, give a report at the end of your code with the top 5 hashtags.\n",
    "\n",
    "**Bonus: 5 points if you standardize the hashtags (thinking about capitalization, etc)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26016476",
   "metadata": {},
   "source": [
    "The standardiztion I considered was capitalization where all values of column **hashtags** was changed to lower case. Also, the nan values(missing) was removed, so we only have hashtags that were actually inputed by users, I further did more cleaning so the hastag columns appear as a string rather than a list (a comparison was later done, to see what changed after the cleaning), see code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb18de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            hashtags|          N_hashtags|\n",
      "+--------------------+--------------------+\n",
      "|  ['PfizerBioNTech']|      pfizerbiontech|\n",
      "|['coronavirus', '...|coronavirus,sputn...|\n",
      "|['whereareallthes...|whereareallthesic...|\n",
      "|     ['vaccination']|         vaccination|\n",
      "|['BidenHarris', '...|bidenharris,elect...|\n",
      "|['CovidVaccine', ...|covidvaccine,covi...|\n",
      "|['CovidVaccine', ...|covidvaccine,covi...|\n",
      "|['PfizerBioNTech'...|pfizerbiontech,va...|\n",
      "|['COVID19', 'Covi...|covid19,covidvacc...|\n",
      "|  ['PfizerBioNTech']|      pfizerbiontech|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF = ftrd_data.withColumn(\"H_hashtags\", lower(col(\"hashtags\")))\n",
    "DF = DF.filter(DF['H_hashtags']!='nan')\n",
    "DF = DF.withColumn(\"N_hashtags\",regexp_replace(DF['H_hashtags'], r'[^(a-zA-Z|0-9|,)]',''))\n",
    "DF.select(DF[\"hashtags\"], DF[\"N_hashtags\"]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388f33b",
   "metadata": {},
   "source": [
    "**NOTE**: From the above punctuations was removed from the hashtags, so an hashtag of covid_19 and covid19 are seen as the same, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67982028",
   "metadata": {},
   "source": [
    "The old hashtags columns which was not standardized / cleaned and is then removed from the data, see code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6f5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = DF.drop(DF[\"hashtags\"])\n",
    "DF = DF.drop(DF[\"H_hashtags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8791fb",
   "metadata": {},
   "source": [
    "Since an individual may use more than one hashtags and the goal is to count every distinct hashtag, I splitted the newly created hashtag column by comma and changed its type to array, this was done, so I could use the **explode** function has it only works on a column with its type being an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5840aa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     N_hashtagsArray|\n",
      "+--------------------+\n",
      "|    [pfizerbiontech]|\n",
      "|[coronavirus, spu...|\n",
      "|[whereareallthesi...|\n",
      "|       [vaccination]|\n",
      "|[bidenharris, ele...|\n",
      "|[covidvaccine, co...|\n",
      "|[covidvaccine, co...|\n",
      "|[pfizerbiontech, ...|\n",
      "|[covid19, covidva...|\n",
      "|    [pfizerbiontech]|\n",
      "|           [vaccine]|\n",
      "|[yellowfever, cov...|\n",
      "|[iran, coronaviru...|\n",
      "|      [covidvaccine]|\n",
      "|[covidiots, coron...|\n",
      "|      [fda, vaccine]|\n",
      "|    [pfizerbiontech]|\n",
      "|           [vaccine]|\n",
      "|[thankyounhs, pfi...|\n",
      "|[stayhome, stayat...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L1 = DF.select(split(col(\"N_hashtags\"),\",\").alias(\"N_hashtagsArray\")).drop(\"N_hashtags\")\n",
    "L1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff86406",
   "metadata": {},
   "source": [
    "A new column serving has a counter is then created with all its entries as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb98dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|hshtgct|                 col|\n",
      "+-------+--------------------+\n",
      "|      1|      pfizerbiontech|\n",
      "|      1|         coronavirus|\n",
      "|      1|            sputnikv|\n",
      "|      1|         astrazeneca|\n",
      "|      1|      pfizerbiontech|\n",
      "|      1|             moderna|\n",
      "|      1|             covid19|\n",
      "|      1|whereareallthesic...|\n",
      "|      1|      pfizerbiontech|\n",
      "|      1|         vaccination|\n",
      "|      1|         bidenharris|\n",
      "|      1|        election2020|\n",
      "|      1|        covidvaccine|\n",
      "|      1|             covid19|\n",
      "|      1|      pfizerbiontech|\n",
      "|      1|             moderna|\n",
      "|      1|        covidvaccine|\n",
      "|      1|      covid19vaccine|\n",
      "|      1|                  us|\n",
      "|      1|             pakustv|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L1 = L1.withColumn(\"hshtgct\", lit(1))\n",
    "L2 = L1.select(L1.hshtgct,explode(L1.N_hashtagsArray))\n",
    "L2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d23fb",
   "metadata": {},
   "source": [
    "The above was then converted to an rdd, so the tasks of counting hashtags could be done, see code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c27ddd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsh_tag = L2.select(L2['col'],L2['hshtgct'])\n",
    "Rd = hsh_tag.rdd\n",
    "Rd= Rd.reduceByKey(lambda x, y: x+y)\n",
    "Rd = Rd.sortBy(lambda word: -word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a74af61",
   "metadata": {},
   "source": [
    "The top 5 hashtags are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a12aef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pfizerbiontech', 969), ('covid19', 377), ('vaccine', 233), ('covidvaccine', 181), ('pfizer', 106)]\n"
     ]
    }
   ],
   "source": [
    "print(Rd.collect()[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3885a20",
   "metadata": {},
   "source": [
    "**NOTE**: The dataframe exported as a csv file named **cleaned_data.csv** is the **ftrd_data** because **question 6** wanted a report on hashtags and hence more cleaning was done to remove records of nan (missing) hashtags which was assigned to dataframe **DF** which would have affected the cleaned dataframe **ftrd_data** and all computations from **question 1-5**. It is also saved in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2005fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_to_write = ftrd_data.toPandas().to_csv(None).encode()\n",
    "with s3.open('s3://686springbucket-emmanuel/Assignment6/cleaned_data.csv', 'wb') as f:\n",
    "   \tf.write(bytes_to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
